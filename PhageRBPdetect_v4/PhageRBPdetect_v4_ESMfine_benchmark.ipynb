{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5500d52-04c7-4457-9122-253b490062e2",
   "metadata": {},
   "source": [
    "### Updated PhageRBPdetect_v3_ESMfine_benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c8f563-07d6-43ca-833e-5fa8cf7cf736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T20:04:26.494796Z",
     "iopub.status.busy": "2025-02-04T20:04:26.494057Z",
     "iopub.status.idle": "2025-02-04T22:39:07.217354Z",
     "shell.execute_reply": "2025-02-04T22:39:07.216979Z",
     "shell.execute_reply.started": "2025-02-04T20:04:26.494758Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/miniconda3/envs/embedding/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/victor/miniconda3/envs/embedding/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_26661/3229987308.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74921' max='165483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 74921/165483 1:09:41 < 1:24:14, 17.92 it/s, Epoch 1.36/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.092918</td>\n",
       "      <td>0.873862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PhageRBPdetect (ESM2-fine) - benchmarking\n",
    "@author: dimiboeckaerts\n",
    "@date: 2023-12-19\n",
    "\n",
    "Notes: \n",
    "You will probably want to run this script on a GPU-enabled machine (e.g. Google Colab or Kaggle).\n",
    "The ESM-2 T12 model can run on a single GPU with 16GB of memory.\n",
    "Taken from https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb#scrollTo=3d2edc14.\n",
    "If you want to train the ESM-2 T33 model, you will need a machine with 32GB or more memory.\n",
    "\"\"\"\n",
    "#!pip install evaluate datasets\n",
    "\n",
    "# 0 - SET THE PATHS\n",
    "# ------------------------------------------\n",
    "path = '.'\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "GPU = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 1 - TRAIN & TUNING THE MODEL\n",
    "# ------------------------------------------\n",
    "# define path & model checkpoint\n",
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\" # esm2_t12_35M_UR50D, esm2_t33_650M_UR50D, esm2_t36_3B_UR50D\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# get data until SEPT 2021\n",
    "RBPs = pd.read_csv('../2025_data/annotated_RBPs_2025-01.csv')\n",
    "nonRBPs = pd.read_csv('../2025_data/annotated_nonRBPs_2025-01.csv')\n",
    "nonRBPs_sub = nonRBPs.sample(n=10*RBPs.shape[0], random_state=42)\n",
    "nonRBPs_sub = nonRBPs_sub.reset_index(drop=True)\n",
    "months = ['OCT-2021', 'NOV-2021', 'DEC-2021']\n",
    "to_delete_rbps = [i for i, date in enumerate(RBPs['RecordDate']) if any(x in date for x in months)]\n",
    "rbps_upto2021 = RBPs.drop(to_delete_rbps)\n",
    "rbps_upto2021 = rbps_upto2021.reset_index(drop=True)\n",
    "to_delete_nonrbps = [i for i, date in enumerate(nonRBPs_sub['RecordDate']) if any(x in date for x in months)]\n",
    "nonrbps_upto2021 = nonRBPs_sub.drop(to_delete_nonrbps)\n",
    "nonrbps_upto2021 = nonrbps_upto2021.reset_index(drop=True)\n",
    "RBPseqs = rbps_upto2021['ProteinSeq'].tolist()\n",
    "nonRBPseqs = [seq[:2000] for seq in list(nonrbps_upto2021['ProteinSeq'])]\n",
    "sequences = RBPseqs + nonRBPseqs\n",
    "labels = [1]*rbps_upto2021.shape[0] + [0]*nonrbps_upto2021.shape[0]\n",
    "\n",
    "# process the data\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "\n",
    "# define function for metric\n",
    "metric = load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# finetune the model (takes around 1h per epoch on NVIDIA P100 GPU)\n",
    "nlabels = len(set(labels))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=nlabels)\n",
    "batch_size = 2\n",
    "args = TrainingArguments(\n",
    "    'RBPdetect_ESM2finetune',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# save the model & tokenizer\n",
    "model_path = path+'/RBPdetect_v3_ESMfine_2025'\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2dbe3e9-cc24-4152-8304-8947c782bffd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T07:04:05.685367Z",
     "iopub.status.busy": "2025-02-05T07:04:05.685197Z",
     "iopub.status.idle": "2025-02-05T07:04:46.122943Z",
     "shell.execute_reply": "2025-02-05T07:04:46.122296Z",
     "shell.execute_reply.started": "2025-02-05T07:04:05.685356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.      | 0/3204 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3204/3204 [00:35<00:00, 90.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking F1 Score: 0.9170\n"
     ]
    }
   ],
   "source": [
    "# 2 - BENCHMARKING\n",
    "# ------------------------------------------\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "GPU = 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "device = torch.device(\"cuda\")\n",
    "path = \".\"\n",
    "\n",
    "model_path = path+'/RBPdetect_v3_ESMfine_2025'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval().cuda() # or without cuda if not available\n",
    "\n",
    "RBPs = pd.read_csv('../2025_data/annotated_RBPs_2025-01.csv')\n",
    "nonRBPs = pd.read_csv('../2025_data/annotated_nonRBPs_2025-01.csv')\n",
    "nonRBPs_sub = nonRBPs.sample(n=10*RBPs.shape[0], random_state=42)\n",
    "nonRBPs_sub = nonRBPs_sub.reset_index(drop=True)\n",
    "months = ['OCT-2021', 'NOV-2021', 'DEC-2021']\n",
    "to_delete_rbps = [i for i, date in enumerate(RBPs['RecordDate']) if all(x not in date for x in months)]\n",
    "rbps_2021 = RBPs.drop(to_delete_rbps)\n",
    "rbps_2021 = rbps_2021.reset_index(drop=True)\n",
    "to_delete_nonrbps = [i for i, date in enumerate(nonRBPs_sub['RecordDate']) if all(x not in date for x in months)]\n",
    "nonrbps_2021 = nonRBPs_sub.drop(to_delete_nonrbps)\n",
    "nonrbps_2021 = nonrbps_2021.reset_index(drop=True)\n",
    "testdata = list(rbps_2021['ProteinSeq']) + list(nonrbps_2021['ProteinSeq'])\n",
    "testlabels = [1]*rbps_2021.shape[0] + [0]*nonrbps_2021.shape[0]\n",
    "\n",
    "predictions = []\n",
    "scores = []\n",
    "for sequence in tqdm(testdata):\n",
    "    encoding = tokenizer(sequence, return_tensors=\"pt\", truncation=True).to('cuda:0')#.to('mps:0') # or without cuda if not available\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "        predictions.append(int(output.logits.argmax(-1)))\n",
    "        scores.append(float(output.logits.softmax(-1)[:, 1]))\n",
    "\n",
    "# define function for metric\n",
    "metric = load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "# Compute the F1 score\n",
    "f1_score = metric.compute(predictions=predictions, references=testlabels)\n",
    "\n",
    "# Print the F1 score\n",
    "print(f\"Benchmarking F1 Score: {f1_score['f1']:.4f}\")\n",
    "\n",
    "esm_results = pd.concat([pd.DataFrame(predictions, columns=['preds']), \n",
    "                        pd.DataFrame(scores, columns=['score'])], axis=1)\n",
    "results_path = './RBP_detection'\n",
    "esm_results.to_csv(results_path+'/esm_finetuneT33_test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5abbf188-c586-4d0d-a656-b9411935ad30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T08:53:11.029076Z",
     "iopub.status.busy": "2025-02-05T08:53:11.028761Z",
     "iopub.status.idle": "2025-02-05T08:53:11.047200Z",
     "shell.execute_reply": "2025-02-05T08:53:11.046557Z",
     "shell.execute_reply.started": "2025-02-05T08:53:11.029053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Metrics:\n",
      "  - F1 Score: 0.9170\n",
      "  - MCC Score: 0.9027\n",
      "  - Sensitivity (Recall+): 0.9384\n",
      "  - Specificity (Recall-): 0.9813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = f1_score(testlabels, predictions)\n",
    "\n",
    "# Compute MCC\n",
    "mcc = matthews_corrcoef(testlabels, predictions)\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(testlabels, predictions).ravel()\n",
    "\n",
    "# Compute Sensitivity (Recall for Positive Class)\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "# Compute Specificity (Recall for Negative Class)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "# Print the results\n",
    "print(f\"Benchmarking Metrics:\")\n",
    "print(f\"  - F1 Score: {f1:.4f}\")\n",
    "print(f\"  - MCC Score: {mcc:.4f}\")\n",
    "print(f\"  - Sensitivity (Recall+): {sensitivity:.4f}\")\n",
    "print(f\"  - Specificity (Recall-): {specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b300fcb-1c70-4993-9f8c-118c032eb668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T09:19:51.196479Z",
     "iopub.status.busy": "2025-02-05T09:19:51.196171Z",
     "iopub.status.idle": "2025-02-05T09:19:51.212741Z",
     "shell.execute_reply": "2025-02-05T09:19:51.211997Z",
     "shell.execute_reply.started": "2025-02-05T09:19:51.196454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87% intersection was found between the test data and the train data.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{round(((len(set(testdata).intersection(sequences)))/len(set(testdata)))*100,2)}% intersection was found between the test data and the train data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
